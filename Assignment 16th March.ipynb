{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f99fdf-fd5d-4503-bab0-8f6f235953c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
    "\n",
    "Overfitting -> Occurs when a model learns the training data too well, capturing noise and irrelevant patterns, leading to poor generalization on unseen data. Consequences: High performance on training data but poor performance on test data. Mitigation: Increase training data, feature selection, cross-validation, regularization, and early stopping.\n",
    "\n",
    "Underfitting -> Happens when a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and test data. Consequences: Model fails to capture complexity, high bias, and low variance. Mitigation: Increase model complexity, feature engineering, decrease regularization, ensemble methods, and data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d64479f-38cb-4b77-90e4-6a330297429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2: How can we reduce overfitting? Explain in brief.\n",
    "\n",
    "To reduce overfitting:\n",
    "\n",
    "Increase Training Data: Provide more diverse and representative data to the model.\n",
    "Feature Selection: Choose relevant features and reduce dimensionality.\n",
    "Cross-Validation: Use techniques like k-fold cross-validation to estimate model performance.\n",
    "Regularization: Apply techniques like L1/L2 regularization to penalize overly complex models.\n",
    "Early Stopping: Monitor model performance on a validation set and stop training when performance degrades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741ec86-240b-4c0a-87ef-f3ae13024a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "Scenarios where underfitting can occur in machine learning:\n",
    "\n",
    "Simple Models: When using overly simplistic models that cannot capture the complexity of the data.\n",
    "Insufficient Training Data: When the training dataset is small or not representative of the underlying distribution.\n",
    "Inappropriate Model Complexity: When using models with too few parameters or features to capture the underlying patterns.\n",
    "High Bias: When the model makes strong assumptions about the data that do not hold true.\n",
    "Early Stopping: Stopping the training process too early, preventing the model from learning the underlying patterns effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5974b8b-e519-4aad-b622-4f1dcda5a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n",
    "The bias-variance tradeoff in machine learning is about balancing the simplicity of a model (bias) with its flexibility (variance).\n",
    "\n",
    "High bias leads to underfitting, where the model is too simple to capture patterns in the data.\n",
    "High variance leads to overfitting, where the model is too complex and captures noise in the data.\n",
    "The goal is to find the right balance between bias and variance to develop models that generalize well to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3b741a-760c-46b7-bba4-36e9f14ec4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\n",
    "To detect overfitting and underfitting:\n",
    "\n",
    "1.Validation Curves: Plot training and validation error against model complexity.\n",
    "2.Learning Curves: Plot training and validation error against training data size.\n",
    "3.Cross-Validation: Evaluate model performance across different folds.\n",
    "4.Regularization Effects: Introduce regularization techniques and observe their impact.\n",
    "5.Validation Set Performance: Monitor model performance on a separate validation set during training.\n",
    "\n",
    "To determine if a model is overfitting:\n",
    "\n",
    "Look for a large gap between training and validation error, indicating good performance on training data but poor performance on unseen data.\n",
    "\n",
    "To determine if a model is underfitting:\n",
    "\n",
    "Both training and validation errors are high, indicating the model is too simple to capture patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1850efa5-82eb-4959-901d-4854cf399606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n",
    "\n",
    "Bias:\n",
    "\n",
    "Error from oversimplified assumptions.\n",
    "High bias models are too simple and underfit the data.\n",
    "Example: Linear regression with few features.\n",
    "Variance:\n",
    "\n",
    "Error from sensitivity to training data noise.\n",
    "High variance models are overly complex and overfit the data.\n",
    "Example: High-degree polynomial regression.\n",
    "Comparison:\n",
    "\n",
    "Performance: High bias models perform poorly on both training and test data. High variance models perform well on training data but poorly on test data.\n",
    "Flexibility: High bias models are less flexible, capturing fewer patterns. High variance models are more flexible, capturing more intricate patterns.\n",
    "Risk: High bias models are less risky but may miss important patterns. High variance models are riskier, potentially capturing noise as patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a6ecc5-292c-4471-9dd8-4793770c4184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\n",
    "Regularization in machine learning is a technique used to prevent overfitting by adding a penalty term to the model's loss function. This penalty encourages the model to learn simpler patterns and reduces its reliance on complex features or high coefficients.\n",
    "\n",
    "Common regularization techniques:\n",
    "\n",
    "L1 Regularization (Lasso):\n",
    "\n",
    "Adds the absolute values of the coefficients as a penalty term.\n",
    "Encourages sparsity by shrinking some coefficients to zero.\n",
    "Useful for feature selection.\n",
    "Loss function: Original loss + λ * Σ|θi|\n",
    "L2 Regularization (Ridge):\n",
    "\n",
    "Adds the squared values of the coefficients as a penalty term.\n",
    "Penalizes large coefficients, encouraging them to be small.\n",
    "Helps to prevent overfitting by smoothing the model's decision surface.\n",
    "Loss function: Original loss + λ * Σ(θi^2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
